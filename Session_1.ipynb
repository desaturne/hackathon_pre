{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on Lab Instructions"
      ],
      "metadata": {
        "id": "p_xHocUcZBfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Python Environment"
      ],
      "metadata": {
        "id": "nR4NBdCAY871"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0PKG_5Cw6yl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf190ea-4b9c-45c2-cde3-2a7f9e75611a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4BuipmKZRsP",
        "outputId": "951deaa1-b5f5-4056-d1df-5912f9898e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Text Processing of Sports Commentary"
      ],
      "metadata": {
        "id": "FpODcQNjZFFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sports commentary text\n",
        "commentary = \"\"\"\n",
        "Messi receives the ball at the halfway line. He dribbles past two defenders with incredible skill.\n",
        "He shoots from outside the box... GOAL! What an amazing strike from the Argentine superstar!\n",
        "That's his 20th goal of the season, putting Barcelona ahead 1-0 in this crucial match.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZeaDT03VYjkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic analysis\n",
        "print(\"Original text:\")\n",
        "print(commentary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWSbglS1dmXv",
        "outputId": "86896d5b-4ff7-4895-c942-d1ada1e20fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "\n",
            "Messi receives the ball at the halfway line. He dribbles past two defenders with incredible skill.\n",
            "He shoots from outside the box... GOAL! What an amazing strike from the Argentine superstar!\n",
            "That's his 20th goal of the season, putting Barcelona ahead 1-0 in this crucial match.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "words = word_tokenize(commentary)\n",
        "sentences = sent_tokenize(commentary)"
      ],
      "metadata": {
        "id": "NOMt6VwrdtnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nNumber of words: {len(words)}\")\n",
        "print(f\"Number of sentences: {len(sentences)}\")\n",
        "print(f\"First 10 tokens: {words[:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KkERVI2dv0t",
        "outputId": "ca04e996-dbab-4996-8fb9-35a382e3694c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of words: 54\n",
            "Number of sentences: 5\n",
            "First 10 tokens: ['Messi', 'receives', 'the', 'ball', 'at', 'the', 'halfway', 'line', '.', 'He']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(f\"\\nStemmed words (first 10): {stemmed_words[:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX3RuyfZdzn8",
        "outputId": "1b2575b0-d667-4945-d27e-f321f59efd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemmed words (first 10): ['messi', 'receiv', 'the', 'ball', 'at', 'the', 'halfway', 'line', '.', 'he']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(f\"\\nLemmatized words (first 10): {lemmatized_words[:10]}\")"
      ],
      "metadata": {
        "id": "u-lcXmOOd1uz",
        "outputId": "2a9e9574-887c-4a1f-a632-e9178dc697f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized words (first 10): ['Messi', 'receives', 'the', 'ball', 'at', 'the', 'halfway', 'line', '.', 'He']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(f\"\\nWords after stopword removal (first 10): {filtered_words[:10]}\")"
      ],
      "metadata": {
        "id": "kyaGxjkWd3wx",
        "outputId": "11ab5661-fde1-474b-9df5-3db5a0dd8882",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Words after stopword removal (first 10): ['Messi', 'receives', 'ball', 'halfway', 'line', '.', 'dribbles', 'past', 'two', 'defenders']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spaCy analysis\n",
        "doc = nlp(commentary)\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} - {ent.label_}\")"
      ],
      "metadata": {
        "id": "Wt3KTMK4d6NE",
        "outputId": "197bb66e-9549-4d5d-82d3-8c18344091d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Named Entities:\n",
            "Messi - PERSON\n",
            "two - CARDINAL\n",
            "Argentine - NORP\n",
            "20th - ORDINAL\n",
            "the season - DATE\n",
            "Barcelona - ORG\n",
            "1 - CARDINAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Analysis of Sports Language Patterns"
      ],
      "metadata": {
        "id": "PZOUXQQ5ZmQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze word frequencies\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Clean and normalize text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned_text = clean_text(commentary)\n",
        "cleaned_words = cleaned_text.split()\n",
        "word_freq = Counter(cleaned_words)\n",
        "\n",
        "# Display most common words\n",
        "print(\"\\nMost common words:\")\n",
        "for word, count in word_freq.most_common(10):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Compare with another sport's commentary\n",
        "basketball_commentary = \"\"\"\n",
        "Curry with the ball at the top of the key. He crosses over his defender, steps back behind the arc.\n",
        "Launches a deep three... BANG! Nothing but net! That's his 5th three-pointer tonight,\n",
        "extending the Warriors' lead to 10 points with just 2 minutes remaining in the 4th quarter.\n",
        "\"\"\"\n",
        "\n",
        "# Process basketball commentary similarly\n",
        "# Compare language patterns between the two sports"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qgATGjcZIKh",
        "outputId": "e49c0e10-99ea-4954-a723-25587dae249e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most common words:\n",
            "the: 5\n",
            "he: 2\n",
            "from: 2\n",
            "goal: 2\n",
            "messi: 1\n",
            "receives: 1\n",
            "ball: 1\n",
            "at: 1\n",
            "halfway: 1\n",
            "line: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stopwords"
      ],
      "metadata": {
        "id": "avh-wiGTaW7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords (run once)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Clean and normalize text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Function to process text and return word frequencies (excluding stopwords)\n",
        "def get_word_frequencies(text):\n",
        "    cleaned = clean_text(text)\n",
        "    words = cleaned.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return Counter(filtered_words)\n",
        "\n",
        "# Compare with another sport's commentary\n",
        "basketball_commentary = \"\"\"\n",
        "Curry with the ball at the top of the key. He crosses over his defender, steps back behind the arc.\n",
        "Launches a deep three... BANG! Nothing but net! That's his 5th three-pointer tonight,\n",
        "extending the Warriors' lead to 10 points with just 2 minutes remaining in the 4th quarter.\n",
        "\"\"\"\n",
        "\n",
        "# Process basketball commentary\n",
        "basketball_freq = get_word_frequencies(basketball_commentary)\n",
        "print(\"\\nMost common words (Basketball):\")\n",
        "for word, count in basketball_freq.most_common(10):\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shpqAlXuZpKv",
        "outputId": "e3c01bf6-807c-446a-da6c-a6c9c5c303a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most common words (Basketball):\n",
            "curry: 1\n",
            "ball: 1\n",
            "top: 1\n",
            "key: 1\n",
            "crosses: 1\n",
            "defender: 1\n",
            "steps: 1\n",
            "back: 1\n",
            "behind: 1\n",
            "arc: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini-Project (Starter Code)"
      ],
      "metadata": {
        "id": "UOnYolZ5Eu9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide a function for calculating text statistics\n",
        "def analyze_text_statistics(text, sport_name):\n",
        "    # Calculate various metrics\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Basic statistics\n",
        "    word_count = len([token for token in doc if not token.is_punct])\n",
        "    sentence_count = len(list(doc.sents))\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
        "\n",
        "    # Vocabulary richness (unique words / total words)\n",
        "    unique_words = set([token.text.lower() for token in doc if not token.is_punct])\n",
        "    lexical_diversity = len(unique_words) / word_count if word_count > 0 else 0\n",
        "\n",
        "    # Entity analysis\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    entity_count = len(entities)\n",
        "\n",
        "    return {\n",
        "        \"sport\": sport_name,\n",
        "        \"word_count\": word_count,\n",
        "        \"sentence_count\": sentence_count,\n",
        "        \"avg_sentence_length\": avg_sentence_length,\n",
        "        \"lexical_diversity\": lexical_diversity,\n",
        "        \"entity_count\": entity_count,\n",
        "        \"entities\": entities\n",
        "    }\n",
        "\n",
        "# Students will analyze different sports and compare results"
      ],
      "metadata": {
        "id": "y5TDZ-o6aFGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HOYs1LvKdxlL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}