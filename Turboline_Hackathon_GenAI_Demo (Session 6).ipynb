{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Setup\n",
        "This section sets up the environment for using OpenAI's API. We install the required libraries and configure API access.\n",
        "\n",
        "### Installing Dependencies\n",
        "The following command ensures that we have the latest version of the OpenAI Python library installed:"
      ],
      "metadata": {
        "id": "GKOnCcjbVOEJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY3ylGJ5VJPh",
        "outputId": "897128c7-a271-486b-b450-2b0b248efb9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/734.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/734.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m727.0/734.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.3/734.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up OpenAI Key\n",
        "\n",
        "To interact with OpenAI models, we need an API key. In this example, the API key is retrieved from Google Colab's userdata storage:"
      ],
      "metadata": {
        "id": "k9yZfBL7VWKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "o2DtOlNWVb2a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key variable holds our API key, which will be used for authentication.\n",
        "\n",
        "### Initializing the OpenAI Client\n",
        "\n",
        "We initialize the OpenAI client using our API key:"
      ],
      "metadata": {
        "id": "Yj7t86hzVc4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=key)"
      ],
      "metadata": {
        "id": "xRDnw_7yVlye"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This client will allow us to send requests to OpenAI’s models.\n",
        "\n",
        "\n",
        "## Getting Model Completions\n",
        "\n",
        "This function, `get_completion`, is a core part of our interaction with the OpenAI API. Here's how it works:\n",
        "\n",
        "1. **Prompt:** You provide a text prompt as input. This is your instruction or question for the AI model.\n",
        "2. **Model:** You can specify the model you want to use (default is \"gpt-4o-mini\"). Different models have varying capabilities and costs.\n",
        "3. **Temperature:**  This parameter controls the randomness of the model's output. Higher temperatures make the output more creative, while lower temperatures make it more deterministic.\n",
        "4. **API Call:** The function sends your prompt to the OpenAI API, using your API key for authentication.\n",
        "5. **Response:** The API returns a response containing the model's generated text.\n",
        "6. **Return:** The function extracts the generated text from the response and returns it to you.\n",
        "\n",
        "This function will be used throughout the workshop to get responses from different OpenAI models."
      ],
      "metadata": {
        "id": "sRM2K2-TVpUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-4o-mini\", temperature = 1):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "adMD96bxVvPT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Prompting Principles\n",
        "\n",
        "Prompting principles are essential for effectively interacting with large language models (LLMs) like ChatGPT. By understanding and applying these principles, you can guide the model to generate more accurate, relevant, and creative responses.\n",
        "\n",
        "This section will cover the following key principles:\n",
        "\n",
        "* **Principle 1: Write clear and specific instructions:** Learn how to provide unambiguous instructions to the model to avoid misinterpretations and improve the quality of outputs. We'll explore tactics like using delimiters, asking for structured outputs, and providing clear conditions.\n",
        "* **Principle 2: Give the model time to \"think\":** Discover techniques to help the model process information more thoroughly before generating responses. This includes breaking down tasks into steps and instructing the model to work out solutions before rushing to conclusions.\n",
        "* **Principle 3: Model Limitations: Hallucinations:** Understand that LLMs are not perfect and can sometimes generate outputs that are factually incorrect or nonsensical. We'll discuss ways to mitigate these issues.\n",
        "\n",
        "By mastering these prompting principles, you'll be able to unlock the full potential of LLMs for various tasks like summarization, inference, and transformation."
      ],
      "metadata": {
        "id": "idTR-wFZV0vJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principle 1: Write clear and specific instructions**"
      ],
      "metadata": {
        "id": "A2amcWOzV5T-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input\n",
        "\n",
        "When providing instructions or input to a large language model, it's helpful to use delimiters to clearly separate different parts of the text. Delimiters act as visual cues that help the model understand the structure and boundaries of your input.\n",
        "\n",
        "**What are delimiters?**\n",
        "\n",
        "Delimiters can be any characters or sequences of characters that are not likely to appear within the main text itself. Here are some common examples of delimiters:\n",
        "\n",
        "* Triple backticks:\n",
        "- Triple quotes: \"\"\"  \n",
        "- Angle brackets: < >  \n",
        "- XML tags: \\<tag> \\</tag>  \n",
        "\n",
        "**Why use delimiters?**\n",
        "\n",
        "By using delimiters, you can:\n",
        "\n",
        "- Improve clarity: Make it easier for the model to distinguish between instructions, input text, and expected output.\n",
        "- Reduce ambiguity: Avoid potential misinterpretations or confusion by clearly marking the boundaries of different sections.\n",
        "- Enhance structure: Organize your input into a logical and predictable format that the model can easily process.\n",
        "\n",
        "In the following examples, you'll see how delimiters can be used to effectively guide the model's understanding and response."
      ],
      "metadata": {
        "id": "nL0rb9GlV8Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "You should express what you want a model to do by \\\n",
        "providing instructions that are as clear and \\\n",
        "specific as you can possibly make them. \\\n",
        "This will guide the model towards the desired output, \\\n",
        "and reduce the chances of receiving irrelevant \\\n",
        "or incorrect responses. Don't confuse writing a \\\n",
        "clear prompt with writing a short prompt. \\\n",
        "In many cases, longer prompts provide more clarity \\\n",
        "and context for the model, which can lead to \\\n",
        "more detailed and relevant outputs.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by triple backticks \\\n",
        "into a single sentence.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "VpYJ_uJaV_al",
        "outputId": "c379576f-7562-440f-a127-2f0c2c5037ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To achieve desired outputs from a model, it is essential to provide clear and specific instructions, as longer prompts can enhance clarity and context, reducing the likelihood of irrelevant or incorrect responses.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tactic 2: Ask for a structured output\n",
        "\n",
        "Instead of plain text, you can guide the model to provide output in a structured format like JSON or HTML. This makes it easier to parse and use the model's response programmatically.\n",
        "\n",
        "**Benefits of structured output:**\n",
        "\n",
        "* **Easier processing:** Structured data is readily usable in applications or for further analysis.\n",
        "* **Reduced ambiguity:** Clearly defined data fields minimize chances of misinterpretation.\n",
        "* **Improved consistency:** Ensures that output is formatted uniformly, even for complex data.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "You might ask the model to generate:\n",
        "\n",
        "* A list of items as a JSON array.\n",
        "* A table of data in HTML format.\n",
        "* Key-value pairs in a dictionary-like structure.\n",
        "\n",
        "By requesting structured output, you can tailor the model's response to your specific needs and make it directly usable for downstream tasks."
      ],
      "metadata": {
        "id": "r8E_lNzZWCSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Generate a list of three made-up book titles along \\\n",
        "with their authors and genres.\n",
        "Provide them in JSON format with the following keys:\n",
        "book_id, title, author, genre.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "4jSi9RRwWFoi",
        "outputId": "71522552-3c00-407a-f8b9-9d98466fc647"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'```json\\n[\\n    {\\n        \"book_id\": 1,\\n        \"title\": \"The Whispering Shadows\",\\n        \"author\": \"Lena Nightshade\",\\n        \"genre\": \"Fantasy\"\\n    },\\n    {\\n        \"book_id\": 2,\\n        \"title\": \"Echoes of Tomorrow\",\\n        \"author\": \"Felix Harper\",\\n        \"genre\": \"Science Fiction\"\\n    },\\n    {\\n        \"book_id\": 3,\\n        \"title\": \"The Last Garden\",\\n        \"author\": \"Mira Solstice\",\\n        \"genre\": \"Mystery\"\\n    }\\n]\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tactic 3: Ask the model to check whether conditions are satisfied\n",
        "\n",
        "You can guide the model to evaluate conditions and provide different responses based on whether the conditions are met. This helps in creating more dynamic and interactive prompts.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. **Define conditions:** Clearly state the conditions the model should check for within the input text.\n",
        "2. **Specify actions:** Provide instructions on what the model should do if the conditions are satisfied or not satisfied.\n",
        "\n",
        "**Benefits:**\n",
        "\n",
        "* **Conditional logic:** Enables the model to make decisions based on the input, leading to more tailored responses.\n",
        "* **Improved accuracy:** By checking for conditions, you can ensure the model's response is relevant and aligned with your requirements.\n",
        "* **Enhanced interactivity:** This tactic allows you to create prompts that can adapt to different user inputs."
      ],
      "metadata": {
        "id": "naRi4sacWE02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = f\"\"\"\n",
        "Making a cup of tea is easy! First, you need to get some \\\n",
        "water boiling. While that's happening, \\\n",
        "grab a cup and put a tea bag in it. Once the water is \\\n",
        "hot enough, just pour it over the tea bag. \\\n",
        "Let it sit for a bit so the tea can steep. After a \\\n",
        "few minutes, take out the tea bag. If you \\\n",
        "like, you can add some sugar or milk to taste. \\\n",
        "And that's it! You've got yourself a delicious \\\n",
        "cup of tea to enjoy.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "You will be provided with text delimited by triple quotes.\n",
        "If it contains a sequence of instructions, \\\n",
        "re-write those instructions in the following format:\n",
        "\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\\n",
        "then simply write \\\"No steps provided.\\\"\n",
        "\n",
        "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "p8DUoG9mWMHe",
        "outputId": "7da721cd-26e4-4ae6-d69e-750f7c5c6215"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Step 1 - Get some water boiling.  \\nStep 2 - Grab a cup and put a tea bag in it.  \\nStep 3 - Once the water is hot enough, pour it over the tea bag.  \\nStep 4 - Let it sit for a bit to steep.  \\nStep 5 - After a few minutes, take out the tea bag.  \\nStep 6 - Add sugar or milk to taste if desired.  \\nStep 7 - Enjoy your delicious cup of tea!  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = f\"\"\"\n",
        "The sun is shining brightly today, and the birds are \\\n",
        "singing. It's a beautiful day to go for a \\\n",
        "walk in the park. The flowers are blooming, and the \\\n",
        "trees are swaying gently in the breeze. People \\\n",
        "are out and about, enjoying the lovely weather. \\\n",
        "Some are having picnics, while others are playing \\\n",
        "games or simply relaxing on the grass. It's a \\\n",
        "perfect day to spend time outdoors and appreciate the \\\n",
        "beauty of nature.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "You will be provided with text delimited by triple quotes.\n",
        "If it contains a sequence of instructions, \\\n",
        "re-write those instructions in the following format:\n",
        "\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\\n",
        "then simply write \\\"No steps provided.\\\"\n",
        "\n",
        "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "G1vQhyOqWOwl",
        "outputId": "6c19c9f1-23da-4f28-c295-5dd37041d349"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'No steps provided.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tactic 4: \"Few-shot\" prompting\n",
        "\n",
        "\"Few-shot\" prompting is a technique where you provide the model with a few examples of the desired behavior before giving it the actual task. This helps the model understand the pattern and generate better responses.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. **Provide Examples:** Include a small set of examples demonstrating the desired input-output relationship.\n",
        "2. **Present the Task:** Give the model a new input and ask it to produce the output based on the learned pattern.\n",
        "\n",
        "**Benefits:**\n",
        "\n",
        "* **Improved Performance:** Few-shot prompting can significantly improve the model's accuracy and relevance.\n",
        "* **Reduced Ambiguity:** By showing examples, you clarify the expected format and style of the response.\n",
        "* **Enhanced Generalization:** The model can learn to apply the pattern to new, unseen inputs.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "You might provide the model with a few examples of translating English sentences into French before asking it to translate a new sentence.\n",
        "\n",
        "This technique leverages the model's ability to learn from examples and apply that knowledge to new situations, leading to more accurate and desirable outcomes."
      ],
      "metadata": {
        "id": "HBACMO5OWRmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to answer in a consistent style.\n",
        "\n",
        "<child>: Teach me about patience.\n",
        "\n",
        "<grandparent>: The river that carves the deepest \\\n",
        "valley flows from a modest spring; the \\\n",
        "grandest symphony originates from a single note; \\\n",
        "the most intricate tapestry begins with a solitary thread.\n",
        "\n",
        "<child>: Teach me about resilience.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "k6-7jd1pWVbU",
        "outputId": "9dd110b4-a9ce-44df-981d-d2c970ec795a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<grandparent>: The strongest tree weathers the fiercest storms, for its roots are deep and steadfast; the diamond, born of pressure and time, shines brightest after enduring the darkest depths; the phoenix rises from the ashes, reborn and renewed, embodying the spirit of unwavering strength.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}